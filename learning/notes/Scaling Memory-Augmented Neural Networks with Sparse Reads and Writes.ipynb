{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://papers.nips.cc/paper/6297-scaling-memory-augmented-neural-networks-with-sparse-reads-and-writes\n",
    "Memory networks: the memory is preloaded with information from the task, and then it's only readable.\n",
    "\n",
    "Leaf node: nodes in the bottom.\n",
    "Hamming distance: amount of symbols which two strings differ (hamming_dist(12345, 19346) = 2)\n",
    "Jacobian: suppose f: R^n \\to R^m. Then, the jacobian j \\in R^{n \\times m} has all the first order partial derivatives of each component of the function. In the y-axis we have the derivatives with respect to different entries of the input vector. In the x-axis we have the derivatives of different entries in the output vector. \n",
    "Linked list: each item contains a tuple (value, pointer to next item.) We just need to store the memory position of the head.\n",
    "Circular linked list: in a standard linked list, the last item points to null. Here the last item point to the first item.\n",
    "\n",
    "### K-dimensional tree\n",
    "We create a decision tree where every non-leaf node is a hyperplane separating points. And every leaf node corresponds to a k-dimensional point (or a set of them.) \n",
    "\n",
    "Building process\n",
    "1) Take all the points and select the median in one dimension. \n",
    "2) Create a decision node that separates the points depending on whether they are smaller or greater than the median. (Instead of creating decision nodes, we can use points in the decision nodes.)\n",
    "3) Return to (1) for both child nodes.\n",
    "\n",
    "Insertion process: travel the tree and insert it in the corresponding leaf node (note that this could imbalance the tree)\n",
    "\n",
    "Inference process (exact version)\n",
    "1) Run the decision tree with the query point\n",
    "2) For each node we encounter, compare the distance between the query point and that node. Store the minimium distance.\n",
    "3) Whenever we come across a decision node, we discard the branch we are not taking iff the minimum distance is smaller than the distance from the query point to the hyperplane.\n",
    "\n",
    "Inference process (approximate version)\n",
    "1) Run the decision tree with a query point. If the leaf node has only one point, the return that point. Otherwise, compare the query with every point in the leaf node. \n",
    "\n",
    "The last one is an approximation, for we aren't sure that the query point we assigned to one hypercube doesn't have a nearest point on the other side of the boundaries. (An ensemble of trees would improve in this.)\n",
    "\n",
    "### LSH\n",
    "Building process\n",
    "1) We uniformly, randomly pick several hyperplanes. {why uniformly? wouldn't it make sense to draw planes that relate to the datapoints?}\n",
    "2) We create a hash function that maps every point to the side (left or right) of the hyperplane. (Notice that the hash function range is small: we have only one bit for each hyperplane.)\n",
    "\n",
    "Inference process:\n",
    "1) we put in one bucket every datapoint that has the same result in the hash function. (Not sure :)\n",
    "2) we hash the query point, that will give us one bucket\n",
    "3) we compare the query point with every point in the bucket\n",
    "\n",
    "We can measure the distance of two points using the hamming distance in the result of the hash function. \n",
    "\n",
    "### LSH vs KD trees\n",
    "In KD trees, we have two options: (a) we ask questions about a few axes in the decision nodes, or (b) we ask questions about several axes in the decision nodes.\n",
    "(a) only works if our datapoints are low dimensional. \n",
    "(b) could work in high-dimensions, but the problem is that there are so many combinations of values to ask about that it's near brute force.\n",
    "\n",
    "That's why LSH are used for high-dimensional data.\n",
    "\n",
    "\n",
    "## Time and space complexity\n",
    "We want to prove lower bounds in complexity in algorithms that return a memory m_j with D(q, m_j) < c(1 + e) if a memory m_c s.t. D(q, m_c) = c exists. That is, the algorithm can't do much worse than the minimum of the distances between the query and any given point. The point we retrieve has to be at most at a ce distance of the smallest distance D(q, m_c). {all of this is related to content-based memories. ARe there other ways of doing this?}\n",
    "\n",
    "How do we store the least recently accessed memory? We store a circular linked list with the index of every memory slot. So, moving an item to the back requires O(1) (it's just modifying three items.) And when we read the least recently accessed memory, we move the head by one item to the right. {I don't yet understand how you get to the item to move to the back in O(1)}\n",
    "\n",
    "For backprop, we need to recover the dense memory matrix M at each timestep. For efficiency, we don't store the whole dense memory matrix for each timestep. Instead, we just store the sparse changes between timesteps, and then restore the matrix from M at the last timestep.  \n",
    "\n",
    "This paper tells us that we can reduce complexity time from O(N) to O(log N) (at each timestep) and the space complexity from O(N) to O(1) (at each timestep) without losing performance.\n",
    "\n",
    "### Compressing sparse matrices\n",
    "COO: for each non-zero value, store (column index, row index, value)\n",
    "\n",
    "#### CSR\n",
    "Create three variables\n",
    "* A: all non-zero values\n",
    "* IA: the index of the element in A that corresponds to the first non-zero element in every row.\n",
    "* JA: the column index of every value in A\n",
    "\n",
    "We know all the values in a given row. They are A[IA[i]] to A[IA[i + 1] - 1]. Then we look at JA for the column indexes.\n",
    "\n",
    "(Not sure:) it seems that this method should have something else than just needing 2k instead of 3k (from COO) of storage and computation. It might be that creating M is easier because you can copy entire chunks of rows.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
