{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Networks\n",
    "Summary: transform the input. store it in the memory. retrieve memories that are r\n",
    "\n",
    "Four components:\n",
    "* I: transform input. in: input. out: trans(input)\n",
    "* G: store memories. in: trans(input), m_i and all memories. out: updated m_i\n",
    "* O: retrieve memories. in: trans(input), all memories. out: trans(output)  \n",
    "* R: untransform output. in: trans(output). out: output\n",
    "\n",
    "## G comopnent\n",
    "We store the input in the memories. We can store by categories, and instead of storing everything in a new slot, we can update the previous one in \"real time.\"\n",
    "\n",
    "An efficient way to forget is to remove the memory with the least expected utility.\n",
    "\n",
    "## O component\n",
    "We want to retrieve the most useful n memories. We think of usefulness as how similar is the memory to the input and the previous retrieved memories.\n",
    "\n",
    "$m^n = argmax_{m_i} s_O([input, m^1, ..., m^{n-1}], m_i)$\n",
    "\n",
    "Another option could have been taking the first m_i's that maximize s_O. But conditioning on the already-retrieved memories could give us a better context to continue retrieving memories. (In particular, we could refine the memory a little bit in each time step. For instance, we can start with a blurred memory of a six and we retrieve memories that are more and more clean.)\n",
    "\n",
    "## R component\n",
    "Given the selected memories, now we want to select the word that is most similar to the input and the memories. \n",
    "\n",
    "$r = argmax_{w \\in W} s_R([input, m^1, ..., m^k], w)$\n",
    "\n",
    "A standard algorithm without memory would skip component O and wouldn't condition r on the memories. The hyphothesis that MemNNs are based on is that we only care about so many memories. Thus, having separated steps for selecting memories and using them could be useful.\n",
    "\n",
    "## Score function $s_O$ and $s_R$\n",
    "Before computing the similarity function of the two inputs, we want to map the inputs to a space better suited for computing the similarity. Thus, we define a transformation \n",
    "\n",
    "$\\tilde x = transform(x) = affine(bag\\_of\\_words(x)) = U bag\\_of\\_words(x).$ [#]\n",
    "\n",
    "We then compute the similarity as a dot product between the transformations.\n",
    "\n",
    "$similarity(transform(x), transform(x)) = transform(x)^Ttransform(x)$ \n",
    "\n",
    "{in the paper, they say the bag_of_words(x) function has three dimensions: one to use when the input is y and two to use when the input is x. why don't they use different functions for bag_of_words? I understand that for bag_of_words(x) you need at least D = 2|W| for you have words that come from the input x and from the memory, so we count the words appearance differently. But why don't we have $bag\\_of\\_words_x \\in R^{2|W|}$ and $bag\\_of\\_words_y \\in R^{|W|}$}\n",
    "\n",
    "## Training\n",
    "We have supervised information about the sentences that should have been selected and the response. We use the margin ranking loss that maximizes the distance between the score given to the correct memory location and that given to all the other memory locations. We only care about maximizing the distance up to a certain threshold. For instance, if the distance between the correct memory and an incorrect memory is greater than the threshold, then  the loss remains the same if we multiply that distance by 3.\n",
    "\n",
    "Randomized algorithms help: instead of going through all the other memory locations, we only compute the loss for a subset of the locations.\n",
    "\n",
    "In the following loss we assume we retrieve only two memories in the G step. $m_1$ and $m_2$ are the labels for the memory and $r$ is the label for the response. We want to minimize the following loss.\n",
    "\n",
    "$$\n",
    "L(U_O, U_R) = \\sum_{\\bar m \\neq m_1}max(0, \\gamma - [s_O(x, m_1) - s_O(x, \\bar m)]) + \\\\\n",
    "\\sum_{\\bar m \\neq m_2}max(0, \\gamma - [s_O([x, m_1], m_2) - s_O([x, m_1], \\bar m)]) + \\\\\n",
    "\\sum_{\\bar w \\neq r}max(0, \\gamma - [s_R([x, m_1, m_2], r) - s_R([x, m_1, m_2], \\bar w)])\\\\\n",
    "$$\n",
    "\n",
    "## Segmentation\n",
    "If the input arrives as words instead of sentences, the model needs to learn where to start a new sentence. \n",
    "\n",
    "To do this, we first transform x to an embedding space. Then, we use a linear classifier that based on the sequence of words, it classifies whether it corresponds to a complete sentence or not. If it corresponds, we store that sentence and we start a new sentence with the next word. If it doesn't corresond, we add to the sentence the next word. \n",
    "\n",
    "$linear\\_classifier(transform(x))$\n",
    "\n",
    "## Efficiency\n",
    "### Hashing\n",
    "Requirements:\n",
    "* we have a function f that searches on average on time O(1)\n",
    "* we have a list of vectors A and a vector v\n",
    "* we want to search the vector in A that is most similar to v\n",
    "* we don't want to compare v to every vector in A\n",
    "\n",
    "Process:\n",
    "* We compare v to A_i iff f(v) == f(A_i) \n",
    "\n",
    "An example of this is A being a list of real numbers, f being the floor function, and v being another real number. Thus, we don't compare v to every number in A. Instead, we compute the floor function of v, and compare the result to three buckets: the buckets where $f(v) == f(A_i)$ and the buckets where $f(v) \\pm 1 == f(A_i).$ Notice that if we draw 100 numbers from an uniform distribution between 0 and 10, the complexity is the same as drawing 10n numbers from 0 to n (with n being any natural.) This happens because the complexity of indexing in an array is constant[#].\n",
    "\n",
    "### Implementation\n",
    "#### First option\n",
    "We select the memories that share at least one word with the input.\n",
    "We do this by using\n",
    "* First, using a hash function that maps every word to a single bucket.\n",
    "* Then, we hash the input and every memory. (Notice that we need to compute the hash for the memory only once.) This hash function will return as many buckets as there are words in the sentence.\n",
    "* Finally, we only compare the input to those memories that share at least one word with the sentence. \n",
    "\n",
    "#### Second option\n",
    "Instead of having one bucket for every word, we have k buckets where in each bucket we have similar words. We can obtain this by running k-means. We then consider the memories that share buckets with the input. \n",
    "\n",
    "### Other details\n",
    "Modeling previously unseen words: We can store the words that appear before and after a given word. This can be useful to model unseen words.\n",
    "Exact matches: with the score function, we can't be sure if we have exact matches between question and memory. We can measure the similarity between the bag of words directly, without the embeddings. \n",
    "\n",
    "## Other concepts\n",
    "bag-of-words: a representation of a document where instead of storing the whole document, we store a dictionary with the keys being words and the values being the number of times the words appear in the document.\n",
    "\n",
    "array index is O(1): We know that all the entries of an array are consecutively stored in memory. We also know the datatype of the array (and hence we know the memory size of every entry.) So, if we want to retrieve/store an entry in an array, we do it in O(1) by going to the memory address of first entry, and adding n * memory_size (with n being the position of the value in the array.) \n",
    "\n",
    "## Notes\n",
    "[1] I think that whenever there is something that we can do efficiently, we are taking advantage of some assumption. In this case, we are exploiting the fact that we stored the memories in an organized and predictable way.\n",
    "\n",
    "## Next things\n",
    "\n",
    "{section 3.4 onwards https://arxiv.org/pdf/1410.3916.pdf}\n",
    "Das et al 1992: differntiable nn pushdown automaton.\n",
    "Schmidhuber 1992 and 1993: nn that modifies its own weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
