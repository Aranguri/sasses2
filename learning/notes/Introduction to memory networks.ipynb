{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feedforward neural net resembles a processor that learns. Based on input-output pairs, it deduces the unobservable function that maps inputs to outputs. But much of the data we deal with is sequential. For instance, if we are reading an article, we don't want to neither look at the whole page at once or to read words and forget the previous word every time we read the next one.\n",
    "\n",
    "One option to solve this is a recurrent neural network. Now, it seems we have a processor with a short-term memory. I think it's short-term because in every iteration, the neural net is writing and reading in every position. Also, it's size is relatively small. [1]\n",
    "\n",
    "Naturally, it would be interesting to consider a long-term memory. That is, a large memory which the neural net doesn't fully read/write at every iteration.\n",
    "\n",
    "Consider that we aren't taking into account that memory of our last day in high school to take every decision. However, if you need to decide whether to go to grad school or not, that memory may pop in your mind. It's as if we can query the memory. Somehow, we manage to ... our sea of information. This isn't straightforward with deep learning, for we want to differentiate everything, and if we don't take into account every memory, the model can't learn that it could have been wrong to not include this memory.\n",
    "\n",
    "So the questions are: ...\n",
    "\n",
    "I heard that neural nets are very good at solving problems that we humans can solve in a fraction of second. That is, neural nets can solve perception problems like this is a red car. But it seems that when we can think, we still outperform neural networks (not in everything, consider: alphago, chess.) Why can't a neural network research? what's left?\n",
    "\n",
    "I think that a quality that distinguishes humans from other animals is that we can connect concepts and draw relationships and analogies. In research, I think this happens several times: we take things from one field and apply them to another. We could say that research is about having ideas. And having ideas is about connecting concepts. Thus, we need to store lots of concepts and we need a good retrival system.\n",
    "\n",
    "# Process\n",
    "First we start with a feedforward neural net. But then I realize we need a loop over that fnn - otherwise, we aren't taking into account that the task is sequential.\n",
    "Then, i realize we need a state for the neural net. This happens because the nn receives 0 as input when it's supposed to write. But then, it doesn't know if it's starting to write, or anywhere in the middle\n",
    "Thus, it has to have an state.\n",
    "Now, we have that state. Now I'm testing the nn to see if it learns to use the memory.\n",
    "Indeed, it learned to use the memory. The performance is 5x times better at 800 iterations with memory than without memory.\n",
    "FYI: loss = 0.5 for first 500 iterations. And it takes 1500 iterations to get to 0.02\n",
    "\n",
    "## Copy task\n",
    "The idea of this task is to create a nn that acts as a program that \n",
    "* reads the input, \n",
    "* if we are in the first half: the program writes the input in the memory\n",
    "* if we are in the second half: the program reads from the memory and writes in the output.\n",
    "\n",
    "The difference between a LSTM with memory and a LSTM wo memory is clear when the numbers the LSTM has to remember are more than the dimensionality of its hidden state. \n",
    "\n",
    "## Other task\n",
    "We want to test the model in other task. Which are the capabilities of the agent? It would be nice to have a task that requires a lot of memory, and that we can select the amount of memory as desired. That would be a good environment for a model that doesn't read its whole memory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
