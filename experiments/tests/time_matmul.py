from time import time
import tensorflow as tf
import numpy as np

results = []
times = 10
dim1 = 512
dim2 = 1

A = tf.get_variable('matrix', (times, dim1, dim2))
b = tf.get_variable('vector', (times, dim2, 1))
i = tf.placeholder(tf.int32, ())

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

for i_ in range(times):
    start = time()
    _ = sess.run(tf.matmul(A[i], b[i]), {i: i_})
    results.append(time() - start)

print(results)

'''
# Results
## Exp1
2x2 . 2x1 matmul: [0.301039457321167, 0.004186868667602539, 0.0031118392944335938, 0.003228425979614258, 0.0032737255096435547, 0.004409313201904297, 0.003162860870361328, 0.003144979476928711, 0.0031402111053466797, 0.0033299922943115234]
512x10000 . 10000x1: [0.3481028079986572, 0.009079456329345703, 0.008472204208374023, 0.008511543273925781, 0.00836181640625, 0.008402347564697266, 0.008343696594238281, 0.008217573165893555, 0.008180856704711914, 0.008430242538452148]
512x128 . 128x1: [0.32383108139038086, 0.010026216506958008, 0.009224414825439453, 0.009229898452758789, 0.010218381881713867, 0.00983119010925293, 0.007906913757324219, 0.0076904296875, 0.008000850677490234, 0.0077877044677734375]

512x1 . 1x1: [0.3137190341949463, 0.014557123184204102, 0.02236199378967285, 0.02097463607788086, 0.019144058227539062, 0.0188601016998291, 0.015691041946411133, 0.012050628662109375, 0.011956214904785156, 0.014725208282470703]
512x100 . 100x1: [0.29444003105163574, 0.012474536895751953, 0.011230707168579102, 0.014284610748291016, 0.013735055923461914, 0.013893842697143555, 0.01376652717590332, 0.011467695236206055, 0.014045000076293945, 0.014197349548339844]
512x10000 . 10000x1: [0.32035326957702637, 0.021638870239257812, 0.015556812286376953, 0.014838218688964844, 0.015238285064697266, 0.014721155166625977, 0.014178752899169922, 0.014276504516601562, 0.014320135116577148, 0.014495134353637695]

Conclusion: there is a negligible difference in the cost of computing layers that have size 1 compared to those that have size 10k
'''
